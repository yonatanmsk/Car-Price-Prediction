{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"}\n",
    "headers2 = {\"User-Agent\":\"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; Googlebot/2.1; +http://www.google.com/bot.html) Chrome/41.0.2272.96 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### in this function we take each car URL and fetching the features we needed\n",
    "def singleCarDict(url):\n",
    "    dictionary = {}\n",
    "    soup_cars = BeautifulSoup(requests.get(url, headers=headers).content)\n",
    "    dictionary['Region'] = url.split(\".\")[0].split(\"/\")[2]\n",
    "    car_price = soup_cars.find(\"span\", class_=\"price\")\n",
    "    if car_price is not None:\n",
    "        dictionary['Price'] = car_price.text.replace(',', '').replace('$', '')\n",
    "    year_n_name = soup_cars.find(\"p\", class_=\"attrgroup\").span.get_text()\n",
    "    title = year_n_name.split(\" \", maxsplit=1)\n",
    "    if (title[0]).isnumeric:\n",
    "        dictionary['Year'] = title[0]\n",
    "        dictionary['Model'] = title[1]\n",
    "    else:\n",
    "        dictionary['Model'] = year_n_name\n",
    "    soup_info = BeautifulSoup(requests.get(url, headers=headers).content)\n",
    "    info = soup_info.find_all(\"p\", class_=\"attrgroup\")\n",
    "    clean_info = info[1].get_text()\n",
    "    cut_info = clean_info.split('\\n')\n",
    "    for word in cut_info:\n",
    "        if word.split(\":\")[0] == 'condition':\n",
    "            dictionary['Condition'] = word.split(\":\")[1]\n",
    "        if word.split(\":\")[0] == 'odometer':\n",
    "            dictionary['Odometer'] = word.split(\":\")[1]\n",
    "        if word.split(\":\")[0] == 'fuel':\n",
    "            dictionary['Fuel type'] = word.split(\":\")[1]\n",
    "        if word.split(\":\")[0] == 'transmission':\n",
    "            dictionary['Transmission'] = word.split(\":\")[1]\n",
    "        if word.split(\":\")[0] == 'paint color':\n",
    "            dictionary['Paint color'] = word.split(\":\")[1]\n",
    "        else:\n",
    "            continue\n",
    "    print(dictionary)\n",
    "    return dictionary\n",
    "\n",
    "### in this function we take each region URL and making a list of cars URLs. at the end were making a dataframe for each\n",
    "### region in case we will have a connection error.\n",
    "def regionDF(reg_url):\n",
    "    region_df = pd.DataFrame(columns=['Region', 'Model', 'Year', 'Price', 'Odometer', 'Condition', 'Fuel type', 'Transmission', 'Paint color'])\n",
    "    #a dummy df for efficiency - after every loop of the for loop we merging the dummy df with the main df\n",
    "    url = reg_url + 'd/cars-trucks/search/cta' #chaining the urls with the search cars and trucks url for efficiency\n",
    "    soup_cars_trucks = BeautifulSoup(requests.get(url, headers=headers).content) #making a bs object of the data we feched \n",
    "    button_next = {'href':'/d/cars-trucks/search/cta'}\n",
    "    while button_next is not None:\n",
    "        url = region_url + button_next['href']\n",
    "        soup_cars_trucks = BeautifulSoup(requests.get(url, headers=headers).content) #making a bs object of the data we feched\n",
    "        car_urls = [car_url.get(\"href\") for car_url in soup_cars_trucks.find_all(\"a\", class_=\"result-title\")]\n",
    "        #a for loop to find every single car url\n",
    "        arr=[]\n",
    "        for single_car_url in car_urls:\n",
    "            try:\n",
    "                arr.append(singleCarDict(single_car_url))\n",
    "                time.sleep(3)\n",
    "            except Exception:\n",
    "                pass\n",
    "        region_df = region_df.append(arr, ignore_index=True)\n",
    "        button_next = soup_cars_trucks.find(\"a\", class_=\"button next\") #making an object for the next button\n",
    "#         break\n",
    "    region_df.to_csv(region_df['Region'][0] + '.csv', index=False)\n",
    "    return region_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.craigslist.org/about/sites' #our base url\n",
    "page = requests.get(url, headers=headers)  #getting the url data\n",
    "page_soup = BeautifulSoup(page.content, 'html.parser') #making a bs object of the data we feched\n",
    "url_soup = page_soup.find(\"div\", class_=\"colmask\") #finding all the regions urls\n",
    "url_list=[u['href'] for u in url_soup.find_all(\"a\", href=True)] #making a list of all the region urls\n",
    "main_df = pd.DataFrame(columns=['Region', 'Model', 'Year', 'Price', 'Odometer', 'Condition', 'Fuel type', 'Transmission', 'Paint color']) #our main df\n",
    "for region_url in url_list: #for loop for taking all the cars from all the urls\n",
    "    main_df = main_df.append(regionDF(region_url))\n",
    "#     break\n",
    "main_df.to_csv('CarsDF.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
